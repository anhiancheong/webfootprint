import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import java.io.Reader;
import java.util.ArrayList;
import java.util.HashMap;

import org.json.JSONArray;
import org.json.JSONException;
import org.json.JSONObject;
import org.json.JSONTokener;





/** Each coreExperiment represents a new experiment on the data. Each experiment has unique parameters concerning
 * the websites used, the initial set of information and the various confidence and probability thresholds
 * an experiment is capable of loading the relevant information from a config file 
 * */
public class CoreExperiment {

	
	public double confidenceThreshold = 0.0;
	
	public double aggregateThreshold = 0.0;
	
	public double websiteThreshold = 0.0;
	
	public int experimentID = -1;
	
	/**websitesUsed is a hash map of the possible websites that an experiment could use
	 * each index in the website will look like <googleplus, true>,....
	 * */
	//HashMap<String, Boolean> websitesUsed = new HashMap<String, Boolean>();
	
	/**Arraylist stores the websites being used in this experiment*/
	ArrayList<String> websites = new ArrayList<String>();
	
	/**This is a data strucutre to hold the various cores involved with the experiment.
	 * There is a core of the initial values, an individual core for each website and a final aggregated core
	 * */
	//ArrayList? HashMap?
	//Should each inference procedure be done in another core, or through a method in this class...
	//Each end up being pretty different and it may be better to have a specialized method instead of having
	//a core method with tons of cases and statements
	
	/**These variable hold the first name and last name of the person being examined. Other attribute may be added,
	 * but the names are a minimum for the program to even function therefore are hardcoded */
	public String first_name = "";
	public String last_name = "";
	
	/**These two booleans handle cases for looking at expected values or not*/
	public boolean considerTweets = false;
	public boolean linkedinExtracted = false;
	
	/** This array holds a list of the additional initial attributes that this experiment expects
	 * */
	ArrayList<String> initialAttributes = new ArrayList<String>();
	
	
	/******* PERSON SPECIFIC OBJECTS *******/
	//Each of these will be regenerated for each person that is analyzed
	HashMap<String, CoreAttribute> attributes;
	
	
	
	/** This method will read a json configured file and load the various run parameters
	 *  I just used Tavish's code for this part
	 * */
	public void loadConfig(String filename) {
		try {
			Reader read = null;
			read = new FileReader(new File(filename));
			JSONTokener jsonReader = new JSONTokener(read);
			JSONObject params = new JSONObject(jsonReader);
			/*
			 * Reading in configuration information from the file, things like confidence, merging thresholds etc
			 * */
			System.out.println(params.toString(3));

			confidenceThreshold = (float) params.getDouble("confidenceThreshold_forYifang");
			
			aggregateThreshold = (float) params.getDouble("aggregateThreshold");
			
			websiteThreshold = (float) params.getDouble("individualWebsiteThreshold");
			
			experimentID = params.getInt("experimentId");
			
			linkedinExtracted = params.getBoolean("considerLinkedinExtractedAttrs");
			
			considerTweets = params.getBoolean("considerTweets");
			
			//Getting which websites this experiment will seek to use
			JSONArray temp = params.getJSONArray("websites");

			for (int i = 0; i < temp.length(); i++) {
				websites.add(temp.getString(i));
			}

			

			//Gets the attirbutes for the initial core
			ArrayList<String> initialCoreAttrs = new ArrayList<String>();
			JSONArray tempInit = params.getJSONArray("initialCoreAttributes");

			for (int i = 0; i < tempInit.length(); i++) {
				if (tempInit.getString(i).equals("last_name") || tempInit.getString(i).equals("first_name"))
					continue;
				
				initialAttributes.add(tempInit.getString(i));
			}

			
		}//end of try block
		catch(IOException ie){
			ie.printStackTrace();
		}
		catch(JSONException je){
			je.printStackTrace();
		}
	
	
	}

	/**This method will populate all of the object needed to do the probability and inference task for a given person*/
	public void initialize(String fname, String lname) {
		// TODO Auto-generated method stub
		first_name = fname;
		last_name = lname;
		
		/*
		 * add first and last to a CoreAttribute object
		 * for each other initial attribute, find and populate the CoreAttribute
		 * 
		 */
		attributes = new HashMap<String, CoreAttribute>();
		
		
		attributes.put("first_name", new CoreAttribute("first_name"));
		attributes.get("first_name").addValue(first_name, "ground", 1.0);
		//Sets the isInitialValue of the Attribute to trues
		attributes.get("first_name").isGround();
		
		attributes.put("last_name", new CoreAttribute("last_name"));
		attributes.get("last_name").addValue(last_name, "ground", 1.0);
		attributes.get("last_name").isGround();
		
		
		//For each of the other initial attribute like education or location
		//Add an object to hold their instances, then find their values and 
		//populate the attributes
		for(int i = 0; i < initialAttributes.size(); i++) {
			String groundAttr = initialAttributes.get(i);
			attributes.put(groundAttr, new CoreAttribute(groundAttr));
			populateInitialAttribute(groundAttr);
		}
		
		
	}

	/**Method will query each of our ground truth tables and try to find the correct initial attribute for the given person
	 * */
	private void populateInitialAttribute(String groundAttr) {
		// TODO Auto-generated method stub
		
		/*DO I KNOW GT ID??? LOOK IT UP OR HAVE IT IN FILE*/
		
		// IT WOULD BE BETTER TO LOAD THIS FROM THE FILE AS WELL
		
	}

	/**Run will do the inference and lookup tasks for the person from each table as well as the population engine
	 * It will continue until the attributes become stable
	 * */
	public void run() {
		// TODO Auto-generated method stub
		
		/*
		 * ALGORITHM...
		 * */
		
	}

	public void postToDB() {
		// TODO Auto-generated method stub
		
		/*
		 * Call sub routines to generate all the necessary queries, 
		 * store all the posting queries into an array list, then execute them one by one
		 * 
		 * */
		
	}
	
	
}//end of class
